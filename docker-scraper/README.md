# Docker Scraper - Pipeline Completo Steam + IA

Pipeline dockerizado: Scraping Steam ‚Üí Res√∫menes IA ‚Üí Vectorizaci√≥n ‚Üí SCP autom√°tico.

## üöÄ Setup

```bash
cd /home/g6/reto/docker-scraper
docker compose build
docker compose up -d
```

‚úÖ Ofelia + Scraper listos | ‚è∞ Ejecuta a las **03:00 AM** diarias

## üìä Verificaci√≥n

```bash
docker logs ofelia-scraper-scheduler  # Debe decir "New job registered"
```

## ‚ñ∂Ô∏è Test manual

```bash
docker exec ofelia-scraper-scheduler ofelia-cli run scraper-daily
```

## üìÅ Datos

- **Local**: `volumes/scraper/data/steam-games-data-vect.ndjson`
- **Remoto**: `192.199.1.65:/home/g6/reto/datos/` (SCP autom√°tico)

## ‚öôÔ∏è Configuraci√≥n

### Cambiar hora (editar docker-compose.yml)
```yaml
ofelia.job-exec.scraper-daily.schedule: "0 3 * * *"
```
Ejemplos: `0 2 * * *` (2 AM), `0 3 * * 1` (Lunes 3 AM)

### Windows/Mac (editar .env)
```env
SSH_PATH=${USERPROFILE}/.ssh  # Windows
# SSH_PATH=/Users/usuario/.ssh  # Mac
```

# Para la validaci√≥n (testing r√°pido)

Durante la validaci√≥n, el pipeline tarda mucho (3-4 horas). Para testear r√°pido con pocos juegos:

**Edita `scraper/scripts/sacar-datos-games.py`, busca:**
```python
CANTIDAD_A_PROCESAR = 0  # 0 = procesa todos
```

**Cambia a un n√∫mero peque√±o para testing:**
```python
CANTIDAD_A_PROCESAR = 50  # Solo procesa los primeros 50 juegos (~10 minutos)
# O: CANTIDAD_A_PROCESAR = 100  # (~20 minutos)
```

**Luego actualiza el c√≥digo en Docker:**
```bash
cp -r ../scraper ./scraper
rm -rf ./scraper/{data,logs,backups,.vscode,__pycache__}
docker compose build
docker compose up -d
```

**‚ö†Ô∏è IMPORTANTE**: Cuando termines de validar, **cambia de nuevo a `0`** en el script original antes de hacer commit:
```python
CANTIDAD_A_PROCESAR = 0  # Volver a producci√≥n
```

### Ejecutar inmediatamente al levantar (editar Dockerfile)

**Si quieres ejecutar inmediatamente** (√∫til cuando descargas el proyecto en una m√°quina nueva):

1. Abre `Dockerfile` y cambia la √∫ltima l√≠nea:

```dockerfile
# Actual (espera hasta las 3 AM):
CMD ["tail", "-f", "/dev/null"]

# Cambiar a (ejecuta ahora + espera siguientes ejecuciones):
CMD ["/bin/bash", "-c", "/app/entrypoint.sh && tail -f /dev/null"]
```

2. Reconstruye y levanta:
```bash
docker compose build
docker compose up -d
```

Ahora el contenedor:
- ‚úÖ Ejecuta `/app/entrypoint.sh` inmediatamente (flujo completo)
- ‚úÖ Al terminar, queda esperando las 3 AM para la siguiente ejecuci√≥n autom√°tica

**‚ö†Ô∏è Nota**: Si haces `docker compose restart`, volver√° a ejecutar el flujo completo.


### Actualizar c√≥digo
```bash
cp -r ../scraper ./scraper && rm -rf ./scraper/{data,logs,backups,.vscode,__pycache__}
cp -r ../imp-futuras ./imp-futuras && rm -rf ./imp-futuras/{data,backup,__pycache__}
docker compose build
docker compose up -d
```

## üõë Limpieza

```bash
docker compose down
docker rmi steam-scraper-complete:latest
rm -rf volumes/
```

## üìä Flujo del Pipeline

1. Scraping Steam (`run_pipeline.py`)
2. Filtrado (`filter-games.py`)
3. Res√∫menes IA (`flux.sh` en imp-futuras)
4. Integraci√≥n (`desc-changer.py`)
5. Limpieza tags (`clean-tags.py`)
6. Vectorizaci√≥n (`vectorizador.py`)
7. SCP remoto (autom√°tico)
